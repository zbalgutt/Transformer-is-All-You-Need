# Transformer-is-All-You-Need
Implemented a Tiny Transformer from scratch in PyTorch for next-token prediction on the Tiny Shakespeare corpus. Built core components including token embeddings, positional encoding, causal self-attention, residual connections, and feed-forward layers, and analyzed training dynamics using loss curves, perplexity, and attention heatmaps.
